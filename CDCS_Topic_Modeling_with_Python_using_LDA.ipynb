{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHcc6lJBddYF"
      },
      "source": [
        "# Topic modelling in Python using Gensim\n",
        "\n",
        "Inspired in code by:\n",
        "- Idil Ismiguzel, available at:\n",
        "https://github.com/Idilismiguzel/NLP-with-Python\n",
        "- Christopher S. Corley, available at: https://christop.club/2014/05/06/using-gensim-for-lda/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7-oq5EeddYI"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIx1K8FEuukm",
        "outputId": "db3decf8-9008-479f-dec7-21563916967f"
      },
      "outputs": [],
      "source": [
        "# Data Handling\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 80)\n",
        "import numpy as np\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "%matplotlib inline\n",
        "import matplotlib.patheffects as path_effects\n",
        "import seaborn as sns\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "from gensim.models import CoherenceModel\n",
        "import pyLDAvis.gensim_models\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Python Core\n",
        "from collections import Counter\n",
        "# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "import requests\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGYrAnIDddYL"
      },
      "source": [
        "# Download UN tweets data\n",
        "\n",
        "A little preprocessing is required here to fix a problem in the data file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N-Yhh-aPnDok",
        "outputId": "121655b5-7629-45c3-e628-bae3e9f433a3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/world-politics-datalab/un_hum_rights_office_tweets/main/un_office_humrights_tweets_sept4_2017_sept3_2022.csv'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "\n",
        "open('un_office_humrights_tweets_sept4_2017_sept3_2022.csv', 'wb').write(r.content)\n",
        "\n",
        "# the original file has a problem around row 4037 so we need to import it in two steps to fix it\n",
        "\n",
        "data1 = pd.read_csv(\"un_office_humrights_tweets_sept4_2017_sept3_2022.csv\", header=0, nrows=4037, encoding='utf-8',  quotechar='\"')\n",
        "data1 = data1.iloc[:,:88]\n",
        "\n",
        "!tail -n 17130 un_office_humrights_tweets_sept4_2017_sept3_2022.csv > temp.csv\n",
        "\n",
        "data2 = pd.read_csv(\"temp.csv\", encoding='utf-8',  quotechar='\"', header=None)\n",
        "data2.drop(data2.columns[[14, 15]], axis=1, inplace=True)\n",
        "\n",
        "data2 = pd.DataFrame(data=data2.values, columns=data1.columns)\n",
        "# data_all = data.append(data2,ignore_index=True) # version for use with older versions of pandas\n",
        "data_all = pd.concat((data1, data2), ignore_index=True)\n",
        "\n",
        "# filter out non-English language texts\n",
        "data = data_all.loc[data_all['lang'] == \"en\"].copy()\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYGnkJ_MluYn"
      },
      "source": [
        "# Text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "JnDuZj6b2rGY",
        "outputId": "e78ff2f3-005d-4d87-fef9-db8b540af701"
      },
      "outputs": [],
      "source": [
        "\n",
        "tweets = data[[\"text\"]]\n",
        "tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-rlR2TRlzqY"
      },
      "source": [
        "# Data Cleaning and Preprocessing\n",
        "\n",
        "1. Expand contractions\n",
        "2. Remove links, html tags, numbers, and other unwanted characters\n",
        "3. Tokenise\n",
        "4. Lemmatise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9mr3QaBxL5c0"
      },
      "outputs": [],
      "source": [
        "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "contractions = {\n",
        "    \"ain't\": \"am not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\", # can not?\n",
        "    \"can't've\": \"cannot have\", # can not have?\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"i'd\": \"i would\",\n",
        "    \"i'll\": \"i will\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"that'd\": \"that would\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there'd\": \"there had\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you're\": \"you are\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7CLZG4mT6vF2"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
        "    # Convert words to lower case\n",
        "    text = text.lower()\n",
        "\n",
        "    # Expand contractions\n",
        "    text = text.split()\n",
        "    new_text = []\n",
        "    for word in text:\n",
        "        if word in contractions:\n",
        "            new_text.append(contractions[word])\n",
        "        else:\n",
        "            new_text.append(word)\n",
        "    text = \" \".join(new_text)\n",
        "\n",
        "    # Format words and remove unwanted characters\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text)\n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "\n",
        "    # remove numbers\n",
        "    text = re.sub(r'[0-9]+', ' ', text)\n",
        "\n",
        "    # remove <U+ >\n",
        "    text = re.sub(r'<u.*?>', \" \", text)\n",
        "\n",
        "    text = re.sub(r'<', ' ', text)\n",
        "    text = re.sub(r'>', ' ', text)\n",
        "\n",
        "    # Split documents into tokens\n",
        "    text = nltk.WordPunctTokenizer().tokenize(text)\n",
        "    \n",
        "    # Lemmatize each word: that is, convert inflected forms of words into their reference form, e.g. 'had' becomes 'have'\n",
        "    text = [nltk.stem.WordNetLemmatizer().lemmatize(token, pos='v') for token in text if len(token)>1]\n",
        "\n",
        "    return text\n",
        "\n",
        "def to_string(text):\n",
        "    # Convert list to string\n",
        "    text = ' '.join(map(str, text))\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHEMFfGg6vjq",
        "outputId": "bcd9d8df-34f9-4854-b112-077a142c36d2"
      },
      "outputs": [],
      "source": [
        "data['text_Clean_List'] = data['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jGbETs7VnDow",
        "outputId": "a1eb9d57-7fd5-4e3f-ccdc-4c6f8c6a83ad"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5tmyNpiS8L2",
        "outputId": "9a109479-da68-4eab-fe03-ef6d7ccb955d"
      },
      "outputs": [],
      "source": [
        "data['text_Clean'] = data['text_Clean_List'].apply(to_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['text'][5474]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "VSMyghOsK0-R",
        "outputId": "2b38e4b0-b01d-44d7-d6ef-7b965f1a1f99"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', 15000)\n",
        "data[[\"text\", \"text_Clean\"]].sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "O12ueWi1UiV_",
        "outputId": "b70b3f02-e8f9-4b38-c272-121c470dc2e6"
      },
      "outputs": [],
      "source": [
        "# Join  text together\n",
        "tweet_words = ','.join(list(data['text_Clean'].values))\n",
        "\n",
        "# Count each word\n",
        "word_counter = Counter(tweet_words.split())\n",
        "most_frequent = word_counter.most_common(30)\n",
        "\n",
        "fig = plt.figure(1, figsize = (20,10))\n",
        "_ = pd.DataFrame(most_frequent, columns=(\"words\",\"count\"))\n",
        "sns.barplot(x = 'words', y = 'count', data = _, palette = 'winter', hue='words', legend=False)\n",
        "plt.xticks(rotation=45)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yw_FZ1ovUplF"
      },
      "outputs": [],
      "source": [
        "stopwords_list = stopwords.words('english')\n",
        "#stopwords_list.extend([\"say\", \"go\", \"look\", \"come\", \"see\", \"think\", \"get\", \"one\", \"would\", \"like\", \"could\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5Gb1nClfElG",
        "outputId": "0e769aaa-6105-405e-b141-2bb800e66a0c"
      },
      "outputs": [],
      "source": [
        "data['text_Clean_SW_List'] = [[word for word in line if word not in stopwords_list] for line in data['text_Clean_List']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpWkER6GfTX_",
        "outputId": "022282b5-e000-4e9b-bc33-df41930b59d9"
      },
      "outputs": [],
      "source": [
        "data['text_Clean_SW'] = data['text_Clean_SW_List'].apply(to_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "pGEWE0eOfWUA",
        "outputId": "0227e975-678c-4e33-ccc0-aaec8ef767c5"
      },
      "outputs": [],
      "source": [
        "# Join  text together\n",
        "tweet_words_sw = ','.join(list(data['text_Clean_SW'].values))\n",
        "\n",
        "# Count each word\n",
        "word_counter = Counter(tweet_words_sw.split())\n",
        "most_frequent = word_counter.most_common(30)\n",
        "\n",
        "# Bar plot of frequent words\n",
        "fig = plt.figure(1, figsize = (20,10))\n",
        "_ = pd.DataFrame(most_frequent, columns=(\"words\",\"count\"))\n",
        "sns.barplot(x = 'words', y = 'count', data = _, palette = 'winter', hue='words', legend=False)\n",
        "plt.xticks(rotation=45);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q18gDRHz4vXP"
      },
      "source": [
        "# Create dictionary and corpus (Bag-of-Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSU68VHEsH5J",
        "outputId": "c701795b-aa66-4259-92f3-899405564ea5"
      },
      "outputs": [],
      "source": [
        "# Create Dictionary\n",
        "id2word = gensim.corpora.Dictionary(data['text_Clean_SW_List'])\n",
        "print(len(id2word))\n",
        "id2word.filter_extremes(no_below=15, no_above=0.5)\n",
        "print(len(id2word))\n",
        "# Create Corpus: Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in data['text_Clean_SW_List']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1urXu7lCmCrA"
      },
      "source": [
        "# Determining the Number of Topics using coherence score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "0KGHElczOvmx",
        "outputId": "03796444-eaa4-4d2c-9554-76920ecd8946"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compute Coherence Score\n",
        "number_of_topics = []\n",
        "coherence_score = []\n",
        "t_min = 51\n",
        "t_max = 100\n",
        "\n",
        "import datetime\n",
        "\n",
        "a = datetime.datetime.now()\n",
        "\n",
        "for i in range(t_min,t_max + 1):\n",
        "    a = datetime.datetime.now()\n",
        "    lda_model = gensim.models.ldamodel.LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=id2word,\n",
        "        random_state=100,\n",
        "        num_topics=i,\n",
        "        passes=10,\n",
        "        alpha='auto',\n",
        "        eta='auto',\n",
        "        iterations=50,\n",
        "        per_word_topics=True\n",
        "    )\n",
        "\n",
        "\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data['text_Clean_SW_List'], dictionary=id2word, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    number_of_topics.append(i)\n",
        "    coherence_score.append(coherence_lda);\n",
        "    b = datetime.datetime.now()\n",
        "    delta = b - a\n",
        "    print(\"Calculated coherence score for solution with\", i, \" topics. Total time:\", delta)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "czMOgN_7QEj1"
      },
      "outputs": [],
      "source": [
        "topic_coherence = pd.DataFrame({'number_of_topics':number_of_topics, 'coherence_score':coherence_score})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts4-jht3Q6kI"
      },
      "outputs": [],
      "source": [
        "topic_coherence.loc[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "EWIvAkvPRV0H",
        "outputId": "53e38a9e-fbc3-4158-a918-50d0477c9c76"
      },
      "outputs": [],
      "source": [
        "g = sns.lineplot(data=topic_coherence, x='number_of_topics', y='coherence_score')\n",
        "g.set_xticks(range(t_min,t_max, int((t_max - t_min) / 10)))\n",
        "g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHnL_HzvmGt0",
        "tags": []
      },
      "source": [
        "# Topic Modelling with LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UnnAF0BGWSHl"
      },
      "outputs": [],
      "source": [
        "n_topics = 14\n",
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    num_topics=n_topics,\n",
        "    random_state=100,\n",
        "    #update_every=1,\n",
        "    #chunksize=10,\n",
        "    passes=20,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    iterations=600,\n",
        "    per_word_topics=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7aHabWHUiHs",
        "outputId": "e92a9af8-deae-44bf-8085-95b9038d52c2"
      },
      "outputs": [],
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LlzZFpkmM1A"
      },
      "source": [
        "# Visualising with pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "cMkSpvYuUrRC",
        "outputId": "d35c7679-bec6-4356-a661-e6b5abdc5421"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary=lda_model.id2word, sort_topics=False)\n",
        "vis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxyJjwlIddYk"
      },
      "source": [
        "# Predict the topics in a new document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICKNh59FJiFz",
        "outputId": "926999f2-0276-436f-ee5b-19b502c9a2ea"
      },
      "outputs": [],
      "source": [
        "doc = \"children education\"\n",
        "vec_bow = id2word.doc2bow(preprocess_text(doc))\n",
        "\n",
        "vec_topic = lda_model[vec_bow]  # convert the query to LSI space\n",
        "\n",
        "a = list(sorted(lda_model[vec_bow][0], key=lambda x: x[1]))\n",
        "\n",
        "for t in a[::-1]:\n",
        "    print(t[0], t[1], lda_model.print_topic(t[0]))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S84g_DElddYl"
      },
      "source": [
        "# Generate a Document-Topic Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OersZHfsddYn",
        "outputId": "afb1ea2e-3ab5-4267-a4ce-6b4b8e284b19"
      },
      "outputs": [],
      "source": [
        "document_topic_matrix = [list(dict(lda_model.get_document_topics(doc, minimum_probability=0)).values()) for doc in corpus]\n",
        "document_topic_matrix = pd.DataFrame(document_topic_matrix, columns=lda_model.print_topics(-1))\n",
        "document_topic_matrix[\"text\"] = list(tweets[\"text\"])\n",
        "document_topic_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "bertenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
