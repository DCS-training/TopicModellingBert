{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETfE6Pa77xje"
      },
      "source": [
        "# BERTopic + KeyphraseVectorizers for topic modelling\n",
        "\n",
        "Topic modeling is a technique in natural language processing (NLP) that is used to discover latent topics in a collection of texts. The main objective of topic modeling is to extract meaningful information from large volumes of unstructured text data, which can be useful for various applications such as information retrieval, recommendation systems, and content analysis. It\n",
        "involves determining the categories, or topics, within a set of documents and which topics each document is likely to belong to. This is done through unsupervised learning, meaning that no pre-existing labels or topics are needed, only the text from the documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeC1W3Zb-d7q"
      },
      "source": [
        "## BERTopic\n",
        "\n",
        "Most available data is not meant to be processed by machines but is designed for human consumption. Human processing of large amounts of data is very expensive and slow.\n",
        "\n",
        "Computers are becoming better at understanding unstructured text data by using transformers, machine learning models that infer meaning, sentiment, and entities from text, among other things.\n",
        "\n",
        "BERTopic is a leading Python package that utilizes state-of-the-art sentence transformer models and a custom class-based TF-IDF (Term Frequency - Inverse Document Frequency) to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaartenGr/BERTopic/master/images/logo.png\" width=\"40%\">\n",
        "\n",
        "\n",
        "Transformer models, a neural network architecture, offer a solution to the information bottleneck problem faced by traditional encoder-decoder models, leading to superior performance in natural language processing. Traditional encoder-decoder models consist of two parts: an encoder and a decoder. The encoder takes in an input sequence and encodes it into a fixed-size representation, also known as a context vector, that captures the relevant information from the input sequence. The decoder then generates an output sequence based on the context vector and the previously generated output tokens. One of the main challenges of traditional encoder-decoder models is the information bottleneck problem. Since the input sequence is compressed into a fixed-size context vector, the model may lose some important information from the original sequence. This can lead to poor performance, especially when dealing with long input sequences.\n",
        "\n",
        "The transformer model employs attention mechanisms and several key components, including positional encoding, self-attention, and multi-head attention. Attention mechanisms and positional encoding are key components of modern transformer models, which have become the state-of-the-art in many natural language processing tasks. Positional encoding is used to provide the model with information about the relative position of the tokens in the input sequence. This is important because traditional neural networks treat each input token independently and have no notion of order. Self-attention is a mechanism that allows the model to attend to different parts of the input sequence based on their importance. This is done by computing a weighted sum of the input tokens, where the weights are learned by the model during training. Multi-head attention is an extension of self-attention that allows the model to attend to multiple parts of the input sequence simultaneously. By combining these mechanisms, transformer models are able to capture long-range dependencies in the input sequence and achieve state-of-the-art performance on a wide range of natural language processing tasks.\n",
        "\n",
        "Transformer models generalize better and enable the development of pre-trained models that can be easily adapted to different use cases. However, these models did not provide an accurate method for building sentence-level embeddings until the development of sentence transformers.Unlike traditional transformer models that operate on individual words or tokens, sentence transformer models are designed to encode entire sentences or documents into fixed-length vector representations that can be used for downstream tasks. This is achieved by using a pre-trained transformer encoder that learns to capture contextual relationships between words within a sentence and produce a sentence-level representation that captures the meaning and semantic relationships of the entire sentence.\n",
        "\n",
        "These sentence transformer models have human-like language comprehension skills that can help organize unstructured text data into topics. This process is called topic modeling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWBcJETN-iKA"
      },
      "source": [
        "\n",
        "## KeyphraseVectorizers\n",
        "\n",
        "To understand the main idea of a text quickly, we can use keyphrases. Keyphrases are brief and reflect the meaning of the text. Unlike single words, keyphrases consist of multiple words that describe the most critical aspect of the text, such as \"youth football training\" instead of just \"football\". Keyphrases are better than single keywords because they give a more precise description of the text. The word order in a sentence is crucial for both its grammar and meaning. A collocation is a unique phrase that carries a meaning beyond the literal interpretation of its individual words. For example, \"white house\" holds a special connotation, whereas \"yellow house\" does not.\n",
        "\n",
        "N-gram phrases (where `n` stands for the number of words)  play a fundamental role in natural language processing and text mining, such as parsing, machine translation, and information retrieval. Generally, phrases convey more information than their individual words, making them critical in determining the topics of collections.Fortunately, there are open source tools available that can automatically extract keyphrases from text, and these tools don't need labeled data.\n",
        "\n",
        "In the context of topic modelling, keyphrase vectorizers generate grammatically correct keyphrases to describe topics, instead of just using simple n-grams. This way, important topic description keyphrases will not be missed by setting the n-gram range too short. Additionally, there is no need to remove stopwords in advance, and the resulting topic models are more precise, avoiding keyphrases that are slightly off-topic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIYp5Sqe-aLP"
      },
      "source": [
        "# Enabling the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "[Reference](https://colab.research.google.com/notebooks/gpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo4t3hk5_AOh"
      },
      "source": [
        "# **Installing BERTopic and KeyphraseVectorizers**\n",
        "\n",
        "We start by installing BERTopic and KeyphraseVectorizers from PyPi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTMu0VKL--tX",
        "outputId": "a3cc714d-92e5-42f5-bdc0-36dc556fc0c2"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic\n",
        "!pip install keyphrase-vectorizers\n",
        "!pip install nbformat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b45TZzqV_PWX"
      },
      "source": [
        "# Data: UN Tweets\n",
        "\n",
        "For this example, we use the UN tweets dataset which contains roughly 8500 tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NuYjpkKOjWzd",
        "outputId": "fbd78994-27bd-4487-f058-171b7a3a16c3"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# !wget https://raw.githubusercontent.com/world-politics-datalab/un_hum_rights_office_tweets/main/un_office_humrights_tweets_sept4_2017_sept3_2022.csv\n",
        "!curl -O https://raw.githubusercontent.com/world-politics-datalab/un_hum_rights_office_tweets/main/un_office_humrights_tweets_sept4_2017_sept3_2022.csv \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# the original file has a problem around row 4037 so we need to import it in two steps to fix it\n",
        "\n",
        "data = pd.read_csv(\"un_office_humrights_tweets_sept4_2017_sept3_2022.csv\", header=0, nrows=4037, encoding='utf-8',  quotechar='\"')\n",
        "data = data.iloc[:,:88]\n",
        "\n",
        "!tail -n 17130 un_office_humrights_tweets_sept4_2017_sept3_2022.csv > temp.csv\n",
        "\n",
        "data2 = pd.read_csv(\"temp.csv\", encoding='utf-8',  quotechar='\"', header=None)\n",
        "data2.drop(data2.columns[[14, 15]], axis=1, inplace=True)\n",
        "\n",
        "data2 = pd.DataFrame(data=data2.values, columns=data.columns)\n",
        "\n",
        "# data_all = data.append(data2,ignore_index=True) # version for use with older versions of pandas\n",
        "data_all = pd.concat((data, data2), ignore_index=True)\n",
        "data_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iyWqufYyTxxR"
      },
      "outputs": [],
      "source": [
        "data_all.to_csv(\"un_tweets_corrected.csv\", encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL9soEUc_2e5"
      },
      "source": [
        "Finally, we select only the English tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5b2rnS3Q3NN",
        "outputId": "bb18d379-c18c-4893-f0f8-f5846c65c3bf"
      },
      "outputs": [],
      "source": [
        "en_data = data_all.loc[data_all['lang'] == \"en\"]\n",
        "docs = list(en_data[\"text\"])\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnvYAkgJ_2iY"
      },
      "source": [
        "# Topic Modeling\n",
        "\n",
        "In this example, we will go through the main components of BERTopic and the steps necessary to create a strong topic model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF7_Uj-wDS4T"
      },
      "source": [
        "## Training\n",
        "\n",
        "We start by instantiating BERTopic. We set language to `english` since our documents are in the English language.\n",
        "\n",
        "We will use the  `all-mpnet-base-v2` sentence transformer model.\n",
        "\n",
        "We will also calculate the topic probabilities. However, this can slow down BERTopic significantly at large amounts of data (>100_000 documents). It is advised to turn this off if you want to speed up the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "26daa044e65645d8a286000bf537b954",
            "28055a3fd53642fd84eea0355cfc9ac5",
            "6dcb4bdae7aa4bb19b885359e40dfb4f",
            "9de2c24685724ce2b1a790dd8873870d",
            "7931f4957b9e465a80c325c63c7f8bef",
            "0c59db7b0b064df9abe339b4c1fb331d",
            "440d78e56961425b8b4210af8b601513",
            "1b903bb356a74880b755e23dd589ff4b",
            "a68a47c900064013b31c4276f9c95c4a",
            "f116dced6aeb45679e226e30812db9f8",
            "5dc6232791424236b287ada77ef749f0"
          ]
        },
        "id": "T5GQu98cjZ2p",
        "outputId": "a07b2f15-fb39-4561-e4e2-9b0a80cc06eb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# NOTE: earlier versions of KeyphraseCountVectorizer do not use the `decay` and `delete_min_df` params: delete these if you get an error\n",
        "try:\n",
        "    topic_model = BERTopic(embedding_model=\"all-mpnet-base-v2\",\n",
        "                        language=\"english\",\n",
        "                        calculate_probabilities=True,\n",
        "                        verbose=True,\n",
        "                        vectorizer_model=KeyphraseCountVectorizer(max_df = int(0.5*len(docs)), min_df = 5, decay = 0.1, delete_min_df=5.0)\n",
        "                        )\n",
        "except TypeError:\n",
        "    topic_model = BERTopic(embedding_model=\"all-mpnet-base-v2\",\n",
        "                        language=\"english\",\n",
        "                        calculate_probabilities=True,\n",
        "                        verbose=True,\n",
        "                        vectorizer_model=KeyphraseCountVectorizer(max_df = int(0.5*len(docs)), min_df = 5)\n",
        "                        )\n",
        "topics, probs = topic_model.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlqkYLjzDtLQ"
      },
      "source": [
        "## Extracting Topics\n",
        "After fitting our model, we can start by looking at the results. Typically, we look at the most frequent topics first as they best represent the collection of documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "u5ZJaAvrkOtk",
        "outputId": "d80ad8bd-104e-4aba-8fff-7f17eed764a2"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z-cpxO4rQiz2",
        "outputId": "3b8538e6-a43d-4503-f9fc-9e4868c7eb04"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic_info().iloc[:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD0OckatD1KW"
      },
      "source": [
        "-1 refers to all outliers and should typically be ignored. Next, let's take a look at a frequent topic that were generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3ozxWgrkRAS",
        "outputId": "12d6dbe2-f794-44ac-d259-5f2f9062f2f3"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic(topic=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DJiF8QPHtCH"
      },
      "source": [
        "**NOTE**: BERTopic is stochastic, which means that the topics might differ across runs. This is mostly due to the stochastic nature of UMAP.\n",
        "\n",
        "To see a small set of documents representative of a given topic, use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9NceLHjkTPn",
        "outputId": "984004e9-10ea-4e72-8e25-30348fadc337"
      },
      "outputs": [],
      "source": [
        "topic_model.get_representative_docs(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KZhd80VbkVda",
        "outputId": "0d5f329a-a4ee-4533-813d-84e880ad6463"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'topic': topics, 'document': docs})\n",
        "\n",
        "en_data[\"topic\"] = topics\n",
        "en_data.loc[en_data[\"topic\"] == 5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpeN9pB6LrVe"
      },
      "source": [
        "## Attributes\n",
        "\n",
        "There are a number of attributes that you can access after having trained your BERTopic model:\n",
        "\n",
        "\n",
        "| Attribute | Description |\n",
        "|------------------------|---------------------------------------------------------------------------------------------|\n",
        "| topics_               | The topics that are generated for each document after training or updating the topic model. |\n",
        "| probabilities_ | The probabilities that are generated for each document if HDBSCAN is used. |\n",
        "| topic_sizes_           | The size of each topic                                                                      |\n",
        "| topic_mapper_          | A class for tracking topics and their mappings anytime they are merged/reduced.             |\n",
        "| topic_representations_ | The top *n* terms per topic and their respective c-TF-IDF values.                             |\n",
        "| c_tf_idf_              | The topic-term matrix as calculated through c-TF-IDF.                                       |\n",
        "| topic_labels_          | The default labels for each topic.                                                          |\n",
        "| custom_labels_         | Custom labels for each topic as generated through `.set_topic_labels`.                                                               |\n",
        "| topic_embeddings_      | The embeddings for each topic if `embedding_model` was used.                                                              |\n",
        "| representative_docs_   | The representative documents for each topic if HDBSCAN is used.                                                |\n",
        "\n",
        "For example, to access the predicted topics for the first 10 documents, we simply run the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edVJ98KjIA8v",
        "outputId": "03126ba7-b8c3-43e6-d1af-d1682af1bd15"
      },
      "outputs": [],
      "source": [
        "topic_model.topics_[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhbFdeCDL0Mo"
      },
      "source": [
        "# **Visualization**\n",
        "There are several visualization options available in BERTopic, namely the visualization of topics, probabilities and topics over time. Topic modeling is, to a certain extent, quite subjective. Visualizations help understand the topics that were created."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFVosmpEMHnp"
      },
      "source": [
        "## Visualize Topics\n",
        "After having trained our `BERTopic` model, we can iteratively go through perhaps a hundred topic to get a good\n",
        "understanding of the topics that were extract. However, that takes quite some time and lacks a global representation.\n",
        "Instead, we can visualize the topics that were generated in a way very similar to\n",
        "[LDAvis](https://github.com/cpsievert/LDAvis):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "EUdVgydikZCd",
        "outputId": "5805a2be-1f00-4d9e-bfa4-cda66a55d82f"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWN_Fii3MZDz"
      },
      "source": [
        "## Visualize Topic Probabilities\n",
        "\n",
        "The variable `probabilities` that is returned from `transform()` or `fit_transform()` can\n",
        "be used to understand how confident BERTopic is that certain topics can be found in a document.\n",
        "\n",
        "To visualize the topic probability distributions for document `d`, we simply call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "66s6PMGSMahY",
        "outputId": "35a2cb39-4705-404a-acab-86a69e51dd21"
      },
      "outputs": [],
      "source": [
        "d = 301\n",
        "print(\"Document text:\", docs[d])\n",
        "topic_model.visualize_distribution(probs[d], min_probability=0.005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kal8dXBVNUqJ"
      },
      "source": [
        "## Visualize Topic Hierarchy\n",
        "\n",
        "The topics that were created can be hierarchically reduced. In order to understand the potential hierarchical structure of the topics, we can use scipy.cluster.hierarchy to create clusters and visualize how they relate to one another. This might help selecting an appropriate nr_topics when reducing the number of topics that you have created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C0T066V7icsK",
        "outputId": "d3732bec-6491-4714-84e5-b1b8a14489df"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_hierarchy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaQdnu5UNd1S"
      },
      "source": [
        "## Visualize Terms\n",
        "\n",
        "We can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation. Insights can be gained from the relative c-TF-IDF scores between and within topics. Moreover, you can easily compare topic representations to each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IxgyQylQiUU7",
        "outputId": "a025ecad-0d6a-4adb-8803-2c6ba77c3cab"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_barchart(range(1,40))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBaa6GI5NiTZ"
      },
      "source": [
        "## Visualize Topic Similarity\n",
        "Having generated topic embeddings, through both c-TF-IDF and embeddings, we can create a similarity matrix by simply applying cosine similarities through those topic embeddings. The result will be a matrix indicating how similar certain topics are to each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "2iliB4WzjQ8G",
        "outputId": "fd5df6d7-422b-491f-f838-5c18d02ce6c0"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_heatmap()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC9f-qTzPi1H"
      },
      "source": [
        "# **Search Topics**\n",
        "After having trained our model, we can use `find_topics` to search for topics that are similar\n",
        "to an input search_term. Here, we are going to be searching for topics that closely relate the\n",
        "search term \"vehicle\". Then, we extract the most similar topic and check the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lP7hd5xe3eQ",
        "outputId": "26950216-fd4c-4df0-e6ef-b594cafeab8c"
      },
      "outputs": [],
      "source": [
        "similar_topics, similarity = topic_model.find_topics(\"children\", top_n=5)\n",
        "similar_topics, similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "tmJAbEAqi7qE",
        "outputId": "0c0fd754-30db-4edf-d24b-d6f0844c91b0"
      },
      "outputs": [],
      "source": [
        "for topic in similar_topics:\n",
        "    print(topic_model.get_topic(topic))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpbqOyRHORwR"
      },
      "source": [
        "# **Topic Representation**\n",
        "After having created the topic model, you might not be satisfied with some of the parameters you have chosen. Fortunately, BERTopic allows you to update the topics after they have been created.\n",
        "\n",
        "This allows for fine-tuning the model to your specifications and wishes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrlHYgsmOS81"
      },
      "source": [
        "## Topic Reduction\n",
        "We can also reduce the number of topics after having trained a BERTopic model. The advantage of doing so,\n",
        "is that you can decide the number of topics after knowing how many are actually created. It is difficult to\n",
        "predict before training your model how many topics that are in your documents and how many will be extracted.\n",
        "Instead, we can decide afterwards how many topics seems realistic:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mA4zsrheY_j",
        "outputId": "e194fd93-6c70-4dc4-f975-fe675f4bcc7e"
      },
      "outputs": [],
      "source": [
        "# Further reduce topics\n",
        "r_topic_model = topic_model\n",
        "r_topic_model.reduce_topics(docs, nr_topics=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "JRfkpgnXeecj",
        "outputId": "cdaad2a9-4ef2-4af9-f685-0582f911153d"
      },
      "outputs": [],
      "source": [
        "r_topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "bertenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c59db7b0b064df9abe339b4c1fb331d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b903bb356a74880b755e23dd589ff4b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26daa044e65645d8a286000bf537b954": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28055a3fd53642fd84eea0355cfc9ac5",
              "IPY_MODEL_6dcb4bdae7aa4bb19b885359e40dfb4f",
              "IPY_MODEL_9de2c24685724ce2b1a790dd8873870d"
            ],
            "layout": "IPY_MODEL_7931f4957b9e465a80c325c63c7f8bef"
          }
        },
        "28055a3fd53642fd84eea0355cfc9ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c59db7b0b064df9abe339b4c1fb331d",
            "placeholder": "​",
            "style": "IPY_MODEL_440d78e56961425b8b4210af8b601513",
            "value": "Batches: 100%"
          }
        },
        "440d78e56961425b8b4210af8b601513": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dc6232791424236b287ada77ef749f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dcb4bdae7aa4bb19b885359e40dfb4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b903bb356a74880b755e23dd589ff4b",
            "max": 249,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a68a47c900064013b31c4276f9c95c4a",
            "value": 249
          }
        },
        "7931f4957b9e465a80c325c63c7f8bef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9de2c24685724ce2b1a790dd8873870d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f116dced6aeb45679e226e30812db9f8",
            "placeholder": "​",
            "style": "IPY_MODEL_5dc6232791424236b287ada77ef749f0",
            "value": " 249/249 [00:52&lt;00:00, 10.89it/s]"
          }
        },
        "a68a47c900064013b31c4276f9c95c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f116dced6aeb45679e226e30812db9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
